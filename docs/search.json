[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Musharraf Mir Akhmadova",
    "section": "",
    "text": "HELLO and WELCOME to my page. In this page you can find information about me and my assignments for the course EPPS6302."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "A. Google Trends Data (Manual Method)\nFor this part of the assignment, I visited the Google Trends website and searched for the terms: Trump, Kamala Harris, and Election. I set the geography to the United States and the time range to Past 5 years. After generating the trends, I downloaded the data as a CSV file.\nThe dataset contains weekly search interest scores for each of the three terms from September 2020 to the present. Each score reflects how popular a term was in a specific week, based on Google’s relative scale from 0 to 100. This scale represents how popular a term was relative to its own highest point of popularity during the period.\nFor example, a value of 100 indicates the week when the search term reached its peak popularity in the U.S. A value of 50 would mean the term was half as popular as it was at its peak. Values like 1, 2, or &lt;1 reflect very low search interest, but they don’t mean only one person searched — just that the term was relatively unpopular in that period.\nFrom the downloaded file, I noticed that:\n\n“Trump” had slightly higher interest than the other two terms during most weeks.\n“Kamala Harris” and “Election” often had very low search interest, often marked as &lt;1.\nThese trends align with expected public attention shifts before and after election periods.\n\n\n\nB. Google Trends Data (R Method)\n\nlibrary(gtrendsR)\n\nTrumpHarrisElection = gtrends(\n  c(\"Trump\", \"Harris\", \"Election\"),\n  onlyInterest = TRUE,\n  geo = \"US\",\n  gprop = \"web\",\n  time = \"today+5-y\",\n  category = 0\n) # last five years\n\nthe_df = TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\n\n\n\n\n\n\n\n\ntg = gtrends(\"Russia\", time = \"all\")\n\n# Example: Russia, Ukraine, Energy\nplot(gtrends(c(\"Russia\"), time = \"all\"))\n\n\n\n\n\n\n\ndata(\"countries\")\n\nplot(gtrends(c(\"Russia\"), geo = \"US\", time = \"all\"))\n\n\n\n\n\n\n\nplot(gtrends(c(\"Russia\"), geo = c(\"US\", \"GB\", \"CN\"), time = \"all\"))\n\n\n\n\n\n\n\ntg_iot = tg$interest_over_time\n\ntct = gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\")\ntct = data.frame(tct$interest_over_time)\n\nplot(gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\"))\n\n\n\n\n\n\n\n\n\n\nC. Saving Data\n\nwrite.csv(the_df, \"googletrends_r.csv\", row.names = FALSE)\nsaveRDS(the_df, \"TrumpHarrisElection.rds\")\n\n\n\nD. Comparing two methods\nThe manual method provides an easy interface to search terms and download CSV files. The file includes one row per week with search interest scores for each term in separate columns.\nThe R method gives a structured dataset where each row represents one term per week. It includes columns like keyword, date, and hits.\nKey differences are as follows:\n\nManual file is simpler, but not suitable for analysis.\nR method is reproducible, automatable, and better for plotting or filtering.\n\nBoth use the same 0–100 scale, but the R version is more flexible for data analysis in R."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Musharraf Mir Akhmadova, and I am a second year PhD student in PPPE program. I am interested in doing research and my research interests are gender economics, child nutrition status and global health.\nBesides my academic journey, I am also a new mom to a 9-month baby girl named Fatima. Fatima motivates me not to give up at times when I feel tired during my studies."
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Website Design Reflection\nThis website was created using Quarto in RStudio as part of the EPPS6302 course. The Quarto Website feature allows for simple yet powerful publishing of professional web pages directly from .qmd files.\nThe website uses the Cosmo theme for a clean and professional appearance. A custom styles.css file is also included to enable additional design control if needed in the future.\nThe navigation bar at the top includes:\n\nHome: the landing page of the website\nAbout: a short bio\nResume: links directly to my uploaded CV in PDF format\nAssignments: links to course assignments like this one\n\nThe site is configured using the _quarto.yml file, which defines layout, theme, and navigation structure.\nThe purpose of this website is to showcase my course progress, academic credentials, and assignments in a structured and accessible way.\n\n\nPodcast Review\nIn this episode of the DataFramed podcast, Richie Cotton talks with Karen Ng, who is the Head of Product at HubSpot. They discuss how humans and AI tools can work together in modern workplaces. Karen has worked at major tech companies like Microsoft and Google, so she brings a lot of experience to the conversation.\nOne of the main ideas in the episode is that not all AI tools are the same. Karen explains the differences between chatbots, co-pilots, and agents:\n\nChatbots are simple tools that answer basic questions.\nCo-pilots are AI systems that support you while you work, like giving suggestions or completing part of a task.\nAgents are more advanced. They can take a task, complete it from start to finish, and act more independently.\n\nKaren believes that AI is here to help humans, not replace them. She says, “Humans lead, and AI accelerates.” This means people should make the important decisions, and AI can help make things faster or easier. For example, if someone works in customer service, AI can help by answering common questions, so the human worker can focus on harder problems.\nShe also talks about how companies should introduce AI step by step. She suggests starting small, testing AI tools on simple tasks, and then checking if they actually make things better. Karen says it’s important to use your company’s own data to train the AI. That way, it understands your work better.\nAnother interesting idea she mentions is called AI Engine Optimization (AEO). It’s similar to SEO (Search Engine Optimization), but instead of helping people find your content through Google, AEO helps AI systems understand and use your content. This is a new idea, but it’s becoming more important as AI tools like ChatGPT and Google Bard are used to answer questions and summarize content.\nWhat I liked about this episode is how clear and practical it was. Even though AI is a big topic, Karen explains it in a simple way. She doesn’t use too much technical language, and she gives good examples that are easy to understand. I also appreciated that she focused on how AI can help real workers, not just replace them.\nOverall, I learned a lot from this podcast. It made me feel more comfortable with the idea of using AI in workplaces. I would recommend this episode to anyone who wants to understand how people and AI can work together in smart and helpful ways — especially in jobs like customer service, sales, or marketing."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This assignment demonstrates how to collect, map, and interpret US Census data using R. We use the tidycensus, tigris, sf, and ggplot2 packages. The focus is on median household income and poverty counts by county in California using the 2023 ACS 5-year estimates.\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n\n# 1) API key (make sure you have your Census API key!)\ncensus_api_key(\"1679bfcd12ef3ba5b6103471f96325933d49aa1f\", install = TRUE, overwrite = TRUE)\n\n[1] \"1679bfcd12ef3ba5b6103471f96325933d49aa1f\"\n\n\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n# 3) Parameters (California)\nstate_abbr &lt;- \"CA\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n# 4) Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n\n\n# 5) Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n# 6) Map median income\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = \"gray\", size = 0.1) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal() + \n  theme(\n  legend.position = c(0.99, 0.99),\n  legend.justification = c(\"right\", \"top\"),\n  legend.title = element_text(size = 11),\n  legend.text = element_text(size = 9),\n  legend.background = element_rect(fill = alpha('white', 0.6), color = NA),\n  plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n  plot.caption = element_text(hjust = 0.5, size = 9)\n)\n\n\n\n\n\n\n\n\nThe choropleth map above visualizes median household income across California counties using data from the 2023 ACS 5-year estimates. Counties are shaded using a viridis color scale, where yellow indicates the highest income levels and dark purple indicates the lowest.\nFrom the map, it is clear that the San Francisco Bay Area (including counties such as Santa Clara, San Mateo, and Marin) has the highest median household incomes, with some exceeding $150,000. These counties stand out in bright yellow. On the other hand, inland and northern counties such as those in the Central Valley or far Northern California tend to have lower median incomes, frequently under $75,000.\n\n# 7) Top and bottom counties by poverty count\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_poverty)) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_poverty) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.3423 ymin: 32.53444 xmax: -114.1312 ymax: 38.7364\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Los Angeles County, C…          1322476       15552 (((-118.6044 33.47855, -…\n 2 San Diego County, Cal…           330602        7963 (((-117.596 33.38779, -1…\n 3 Orange County, Califo…           296493        8509 (((-118.1146 33.74461, -…\n 4 San Bernardino County…           291226        9076 (((-117.8025 33.97555, -…\n 5 Riverside County, Cal…           266955        8729 (((-117.6763 33.88882, -…\n 6 Sacramento County, Ca…           197472        6775 (((-121.8625 38.06795, -…\n 7 Fresno County, Califo…           185717        5965 (((-120.9094 36.7477, -1…\n 8 Kern County, Californ…           168825        6993 (((-120.1944 35.78936, -…\n 9 Alameda County, Calif…           149752        4801 (((-122.3423 37.80556, -…\n10 Santa Clara County, C…           128470        5622 (((-122.2027 37.36305, -…\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.256 ymin: 35.78669 xmax: -115.648 ymax: 42.00076\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Alpine County, Califo…              209          98 (((-120.0724 38.70277, -…\n 2 Sierra County, Califo…              325         202 (((-121.0575 39.53999, -…\n 3 Mono County, Californ…             1441         480 (((-119.6489 38.28912, -…\n 4 Modoc County, Califor…             1717         357 (((-121.4572 41.94994, -…\n 5 Inyo County, Californ…             1928         439 (((-118.79 37.39403, -11…\n 6 Plumas County, Califo…             2075         538 (((-121.497 40.43702, -1…\n 7 Colusa County, Califo…             2332         598 (((-122.7851 39.38298, -…\n 8 Mariposa County, Cali…             2347         449 (((-120.3944 37.67504, -…\n 9 Trinity County, Calif…             2830         755 (((-123.6224 40.9317, -1…\n10 Del Norte County, Cal…             2900         589 (((-124.2175 41.95081, -…\n\n\n\n# 8) Save the data as CSV (optional)\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  },
  {
    "objectID": "assign04.html",
    "href": "assign04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Task Overview\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. The provided base code utilizes the rvest package. We modify it to scrape other tables, clean the data, and suggest a data collection plan for research.\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\n\n\n\nRead and Scrape the Wikipedia Page\n\n# Read the web page\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\n\n# Convert the table to a data frame\nfores &lt;- foreignreserve[[1]]\n\n# View original column names\nnames(fores)\n\n[1] \"Country(as recognized by the U.N.)\" \"Continent\"                         \n[3] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[5] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[7] \"Last reporteddate\"                  \"Ref.\"                              \n\n\n\n\nClean Up the Data\n\n# Assign consistent column names\nnames(fores) &lt;- c(\"Country\", \"Continent\", \n                  \"Forexres_InclGold\", \"Change_InclGold\", \n                  \"Forexres_ExclGold\", \"Change_ExclGold\", \n                  \"Date\", \"Ref\")\n\n# Remove junk rows and unnecessary column\nfores &lt;- fores[-c(1, 2), ]\nfores &lt;- select(fores, -Ref)\n\n# Clean date column (remove footnotes)\nfores &lt;- fores %&gt;%\n  mutate(Date = trimws(str_split_fixed(Date, \"\\\\[\", 2)[,1]))\n\n# Convert numeric columns (remove commas)\nfores &lt;- fores %&gt;%\n  mutate(across(c(Forexres_InclGold, Change_InclGold, \n                  Forexres_ExclGold, Change_ExclGold),\n                ~ as.numeric(str_remove_all(., \",\"))))\n\n# Create Rank based on Forexres_InclGold\nfores &lt;- fores %&gt;%\n  arrange(desc(Forexres_InclGold)) %&gt;%\n  mutate(Rank = row_number())\n\n# Move Rank to the first column\nfores &lt;- fores %&gt;%\n  select(Rank, everything())\n\n# Print top 10 rows with all columns visible\nprint(head(fores, 10), width = Inf)\n\n# A tibble: 10 × 8\n    Rank Country      Continent   Forexres_InclGold Change_InclGold\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n 1     1 China        Asia                  3643149           41079\n 2     2 Japan        Asia                  1324210           19774\n 3     3 Switzerland  Europe                1007710           13935\n 4     4 Russia       Europe/Asia            713300             700\n 5     5 India        Asia                   700236            2334\n 6     6 Taiwan       Asia                   597430            4390\n 7     7 Saudi Arabia Asia                   434547           21728\n 8     8 Hong Kong    Asia                   421400            5126\n 9     9 South Korea  Asia                   415700            4300\n10    10 Brazil       Americas               388571            7465\n   Forexres_ExclGold Change_ExclGold Date       \n               &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;      \n 1           3389306           31221 31 Aug 2025\n 2           1230940           16230 31 Aug 2025\n 3            897295           14490 31 Jul 2025\n 4            434487            1517 26 Sep 2025\n 5            605219            4572 26 Sep 2025\n 6            544300            1071 31 Aug 2025\n 7            434116           21728 7 Nov 2024 \n 8            416216              85 8 Nov 2024 \n 9            410900             790 5 Nov 2024 \n10            344173             361 1 Oct 2024 \n\n\n\n\nSuggest a Web Data Collection Plan for Research\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. We use the rvest, stringr, and tidyverse packages. The focus is on extracting structured tabular data, cleaning variables such as date, and preparing it for research use.\nA simple data collection plan might involve:\n\nObjective: Identify trends in macroeconomic indicators across countries\n\nSources: Wikipedia, World Bank, IMF, TradingEconomics\n\nTools: rvest, httr, jsonlite for APIs\n\nStorage: Save into .csv or SQLite for reproducibility\n\nSchedule: Update monthly or quarterly via scheduled R scripts (using cronR or taskscheduleR)"
  },
  {
    "objectID": "assign04.html#task-overview",
    "href": "assign04.html#task-overview",
    "title": "Assignment 4",
    "section": "",
    "text": "This assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. The provided base code utilizes the rvest package. We modify it to scrape other tables, clean the data, and suggest a data collection plan for research.\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)"
  },
  {
    "objectID": "assign04.html#read-and-scrape-the-wikipedia-page",
    "href": "assign04.html#read-and-scrape-the-wikipedia-page",
    "title": "Assignment 4",
    "section": "Read and Scrape the Wikipedia Page",
    "text": "Read and Scrape the Wikipedia Page\n\n# Read the web page\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\n\n# Convert the table to a data frame\nfores &lt;- foreignreserve[[1]]\n\n# View original column names\nnames(fores)\n\n[1] \"Country(as recognized by the U.N.)\" \"Continent\"                         \n[3] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[5] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[7] \"Last reporteddate\"                  \"Ref.\""
  },
  {
    "objectID": "assign04.html#clean-up-the-data",
    "href": "assign04.html#clean-up-the-data",
    "title": "Assignment 4",
    "section": "Clean Up the Data",
    "text": "Clean Up the Data\n\n# Assign consistent column names\nnames(fores) &lt;- c(\"Country\", \"Continent\", \n                  \"Forexres_InclGold\", \"Change_InclGold\", \n                  \"Forexres_ExclGold\", \"Change_ExclGold\", \n                  \"Date\", \"Ref\")\n\n# Remove junk rows and unnecessary column\nfores &lt;- fores[-c(1, 2), ]\nfores &lt;- select(fores, -Ref)\n\n# Clean date column (remove footnotes)\nfores &lt;- fores %&gt;%\n  mutate(Date = trimws(str_split_fixed(Date, \"\\\\[\", 2)[,1]))\n\n# Convert numeric columns (remove commas)\nfores &lt;- fores %&gt;%\n  mutate(across(c(Forexres_InclGold, Change_InclGold, \n                  Forexres_ExclGold, Change_ExclGold),\n                ~ as.numeric(str_remove_all(., \",\"))))\n\n# Create Rank based on Forexres_InclGold\nfores &lt;- fores %&gt;%\n  arrange(desc(Forexres_InclGold)) %&gt;%\n  mutate(Rank = row_number())\n\n# Move Rank to the first column\nfores &lt;- fores %&gt;%\n  select(Rank, everything())\n\n# Print top 10 rows with all columns visible\nprint(head(fores, 10), width = Inf)\n\n# A tibble: 10 × 8\n    Rank Country      Continent   Forexres_InclGold Change_InclGold\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n 1     1 China        Asia                  3643149           41079\n 2     2 Japan        Asia                  1324210           19774\n 3     3 Switzerland  Europe                1007710           13935\n 4     4 Russia       Europe/Asia            713300             700\n 5     5 India        Asia                   700236            2334\n 6     6 Taiwan       Asia                   597430            4390\n 7     7 Saudi Arabia Asia                   434547           21728\n 8     8 Hong Kong    Asia                   421400            5126\n 9     9 South Korea  Asia                   415700            4300\n10    10 Brazil       Americas               388571            7465\n   Forexres_ExclGold Change_ExclGold Date       \n               &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;      \n 1           3389306           31221 31 Aug 2025\n 2           1230940           16230 31 Aug 2025\n 3            897295           14490 31 Jul 2025\n 4            434487            1517 26 Sep 2025\n 5            605219            4572 26 Sep 2025\n 6            544300            1071 31 Aug 2025\n 7            434116           21728 7 Nov 2024 \n 8            416216              85 8 Nov 2024 \n 9            410900             790 5 Nov 2024 \n10            344173             361 1 Oct 2024"
  },
  {
    "objectID": "assign04.html#web-data-collection-plan-for-research",
    "href": "assign04.html#web-data-collection-plan-for-research",
    "title": "Assignment 4",
    "section": "Web Data Collection Plan for Research",
    "text": "Web Data Collection Plan for Research\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. We use the rvest, stringr, and tidyverse packages. The focus is on extracting structured tabular data, cleaning variables such as date, and preparing it for research use.\nA simple data collection plan might involve:\n\nObjective: Identify trends in macroeconomic indicators across countries\n\nSources: Wikipedia, World Bank, IMF, TradingEconomics\n\nTools: rvest, httr, jsonlite for APIs\n\nStorage: Save into .csv or SQLite for reproducibility\n\nSchedule: Update monthly or quarterly via scheduled R scripts (using cronR or taskscheduleR)"
  },
  {
    "objectID": "assign04.html#suggest-a-web-data-collection-plan-for-research",
    "href": "assign04.html#suggest-a-web-data-collection-plan-for-research",
    "title": "Assignment 4",
    "section": "Suggest a Web Data Collection Plan for Research",
    "text": "Suggest a Web Data Collection Plan for Research\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. We use the rvest, stringr, and tidyverse packages. The focus is on extracting structured tabular data, cleaning variables such as date, and preparing it for research use.\nA simple data collection plan might involve:\n\nObjective: Identify trends in macroeconomic indicators across countries\n\nSources: Wikipedia, World Bank, IMF, TradingEconomics\n\nTools: rvest, httr, jsonlite for APIs\n\nStorage: Save into .csv or SQLite for reproducibility\n\nSchedule: Update monthly or quarterly via scheduled R scripts (using cronR or taskscheduleR)"
  },
  {
    "objectID": "assign05.html",
    "href": "assign05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment, we demonstrate how to scrape U.S. Congressional government document metadata using a .csv or .json file from GovInfo.gov.\nWe use R packages like rjson, jsonlite, purrr, and magrittr to parse metadata and download government documents in bulk. This is especially useful for setting up background jobs for automated data collection in political science, public policy, and legal research."
  },
  {
    "objectID": "assign05.html#task-overview",
    "href": "assign05.html#task-overview",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment, we demonstrate how to scrape U.S. Congressional government document metadata using a .csv or .json file from GovInfo.gov.\nWe use R packages like rjson, jsonlite, purrr, and magrittr to parse metadata and download government documents in bulk. This is especially useful for setting up background jobs for automated data collection in political science, public policy, and legal research."
  },
  {
    "objectID": "assign05.html#step-1-load-required-packages",
    "href": "assign05.html#step-1-load-required-packages",
    "title": "Assignment 5",
    "section": "Step 1: Load Required Packages",
    "text": "Step 1: Load Required Packages\nWe begin by loading the required packages and resetting memory. You may need to install some of these using install.packages() before running this.\n\n# Load necessary packages\nlibrary(purrr)\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(rjson)\nlibrary(jsonlite)\n\n\nAttaching package: 'jsonlite'\n\n\nThe following objects are masked from 'package:rjson':\n\n    fromJSON, toJSON\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(readr)\n\n# Reset memory (optional)\ngc(reset = TRUE)\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  720937 38.6    1404464 75.1   720937 38.6\nVcells 1269087  9.7    8388608 64.0  1269087  9.7"
  },
  {
    "objectID": "assign05.html#step-2-read-the-metadata-file-csv-method",
    "href": "assign05.html#step-2-read-the-metadata-file-csv-method",
    "title": "Assignment 5",
    "section": "Step 2: Read the Metadata File (CSV Method)",
    "text": "Step 2: Read the Metadata File (CSV Method)\n\n## CSV method\ngovfiles = read.csv(\n  file = \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\",\n  skip = 2\n)"
  },
  {
    "objectID": "assign05.html#step-3-load-metadata-using-rjson-json-method",
    "href": "assign05.html#step-3-load-metadata-using-rjson-json-method",
    "title": "Assignment 5",
    "section": "Step 3: Load Metadata Using rjson (JSON Method)",
    "text": "Step 3: Load Metadata Using rjson (JSON Method)\nThe JSON file provides the same data in a structured format. Here we use the rjson package and combine the results into a single data frame.\n\n## JSON method: rjson\ngf_list &lt;- rjson::fromJSON(\n  file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\ngovfiles2 = dplyr::bind_rows(gf_list$resultSet)\n\n## JSON method: jsonlite\ngf_list1 = jsonlite::read_json(\n  \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\n\n## Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n## One more step: flatten into a data frame\ngovfiles3 &lt;- govfiles3 |&gt; dplyr::bind_rows()"
  },
  {
    "objectID": "assign05.html#step-4-download-5-sample-reports",
    "href": "assign05.html#step-4-download-5-sample-reports",
    "title": "Assignment 5",
    "section": "Step 4: Download 5 Sample Reports",
    "text": "Step 4: Download 5 Sample Reports\nWe use a simple for loop to download the first five PDFs from govfiles3. These files are saved in the same working directory.\n\nfor (i in 1:5) {\n  download.file(\n    url = govfiles3$pdfLink[i],\n    destfile = paste0(\"report_\", i, \".pdf\")\n  )\n}"
  }
]