[
  {
    "objectID": "assign05.html",
    "href": "assign05.html",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment, we demonstrate how to scrape U.S. Congressional government document metadata using a .csv or .json file from GovInfo.gov.\nWe use R packages like rjson, jsonlite, purrr, and magrittr to parse metadata and download government documents in bulk. This is especially useful for setting up background jobs for automated data collection in political science, public policy, and legal research."
  },
  {
    "objectID": "assign05.html#task-overview",
    "href": "assign05.html#task-overview",
    "title": "Assignment 5",
    "section": "",
    "text": "In this assignment, we demonstrate how to scrape U.S. Congressional government document metadata using a .csv or .json file from GovInfo.gov.\nWe use R packages like rjson, jsonlite, purrr, and magrittr to parse metadata and download government documents in bulk. This is especially useful for setting up background jobs for automated data collection in political science, public policy, and legal research."
  },
  {
    "objectID": "assign05.html#step-1-load-required-packages",
    "href": "assign05.html#step-1-load-required-packages",
    "title": "Assignment 5",
    "section": "Step 1: Load Required Packages",
    "text": "Step 1: Load Required Packages\nWe begin by loading the required packages and resetting memory. You may need to install some of these using install.packages() before running this.\n\n# Load necessary packages\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(data.table)\nlibrary(readr)\n\n# Reset memory (optional)\ngc(reset = TRUE)\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  720843 38.5    1404178   75   720843 38.5\nVcells 1268977  9.7    8388608   64  1268977  9.7"
  },
  {
    "objectID": "assign05.html#step-2-read-the-metadata-file-csv-method",
    "href": "assign05.html#step-2-read-the-metadata-file-csv-method",
    "title": "Assignment 5",
    "section": "Step 2: Read the Metadata File (CSV Method)",
    "text": "Step 2: Read the Metadata File (CSV Method)\n\n## CSV method\ngovfiles = read.csv(\n  file = \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\",\n  skip = 2\n)"
  },
  {
    "objectID": "assign05.html#step-3-load-metadata-using-rjson-json-method",
    "href": "assign05.html#step-3-load-metadata-using-rjson-json-method",
    "title": "Assignment 5",
    "section": "Step 3: Load Metadata Using rjson (JSON Method)",
    "text": "Step 3: Load Metadata Using rjson (JSON Method)\nThe JSON file provides the same data in a structured format. Here we use the rjson package and combine the results into a single data frame.\n\n## JSON method: rjson\ngf_list &lt;- rjson::fromJSON(\n  file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\ngovfiles2 = dplyr::bind_rows(gf_list$resultSet)\n\n## JSON method: jsonlite\ngf_list1 = jsonlite::read_json(\n  \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\n\n## Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n## One more step: flatten into a data frame\ngovfiles3 &lt;- govfiles3 |&gt; dplyr::bind_rows()"
  },
  {
    "objectID": "assign05.html#step-4-download-5-sample-reports",
    "href": "assign05.html#step-4-download-5-sample-reports",
    "title": "Assignment 5",
    "section": "Step 4: Download 5 Sample Reports",
    "text": "Step 4: Download 5 Sample Reports\nWe use a simple for loop to download the first five PDFs from govfiles3. These files are saved in the same working directory.\n\nfor (i in 1:5) {\n  download.file(\n    url = govfiles3$pdfLink[i],\n    destfile = paste0(\"report_\", i, \".pdf\")\n  )\n}"
  },
  {
    "objectID": "assign05.html#implementatiom",
    "href": "assign05.html#implementatiom",
    "title": "Assignment 5",
    "section": "Implementatiom",
    "text": "Implementatiom\n\nStep 1: Load Required Packages\nWe begin by loading the required packages and resetting memory. You may need to install some of these using install.packages() before running this.\n\n# Load necessary packages\nlibrary(purrr)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(data.table)\nlibrary(readr)\n\n# Reset memory (optional)\ngc(reset = TRUE)\n\n          used (Mb) gc trigger (Mb) max used (Mb)\nNcells  720913 38.6    1404095   75   720913 38.6\nVcells 1269454  9.7    8388608   64  1269454  9.7\n\n\n\n\nStep 2: Read the Metadata File (CSV Method)\n\n## CSV method\ngovfiles = read.csv(\n  file = \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\",\n  skip = 2\n)\n\n\n\nStep 3: Load Metadata Using rjson (JSON Method)\nThe JSON file provides the same data in a structured format. Here we use the rjson package and combine the results into a single data frame.\n\n## JSON method: rjson\ngf_list &lt;- rjson::fromJSON(\n  file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\ngovfiles2 = dplyr::bind_rows(gf_list$resultSet)\n\n## JSON method: jsonlite\ngf_list1 = jsonlite::read_json(\n  \"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\"\n)\n\n## Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n## One more step: flatten into a data frame\ngovfiles3 &lt;- govfiles3 |&gt; dplyr::bind_rows()\n\n\n\nStep 4: Download 5 Sample Reports\nWe use a simple for loop to download the first five PDFs from govfiles3. These files are saved in the same working directory.\n\nfor (i in 1:5) {\n  download.file(\n    url = govfiles3$pdfLink[i],\n    destfile = paste0(\"report_\", i, \".pdf\")\n  )\n}\n\n\n\nStep 5: Show preview of scraped metadata\n\nhead(govfiles3[, c(\"title\", \"pdfLink\")])\n\n# A tibble: 6 × 2\n  title                                                                  pdfLink\n  &lt;chr&gt;                                                                  &lt;chr&gt;  \n1 Condemning the brutal Hamas-led terrorist attack on Israel on October… https:…\n2 Providing for congressional disapproval of the proposed foreign milit… https:…\n3 Commemorating the tenth anniversary of the murder of James Wright Fol… https:…\n4 Providing for congressional disapproval of the proposed license amend… https:…\n5 Providing for congressional disapproval of the proposed foreign milit… https:…\n6 Providing for congressional disapproval of the proposed foreign milit… https:…\n\n\n\n\nStep 6: List Downloaded Files\n\nlist.files(pattern = \"report_.*.pdf\")\n\n[1] \"report_1.pdf\" \"report_2.pdf\" \"report_3.pdf\" \"report_4.pdf\" \"report_5.pdf\""
  },
  {
    "objectID": "assign05.html#summary",
    "href": "assign05.html#summary",
    "title": "Assignment 5",
    "section": "Summary",
    "text": "Summary\nIn this assignment, I scraped U.S. government documents using JSON metadata from GovInfo.gov.\nUsing the rjson and jsonlite packages, I extracted metadata, previewed document titles, and batch-downloaded five official PDF reports.\nThe table above shows a preview of document titles and download links, and the messages below confirm the successful downloads."
  },
  {
    "objectID": "assign06.html",
    "href": "assign06.html",
    "title": "Assignment 6",
    "section": "",
    "text": "This assignment demonstrates various text analytics techniques using the quanteda R package. We analyze two datasets:\n\nTweets about the Biden–Xi summit in 2021\nUS Presidential Inaugural Addresses\nAdditionally, we explore the Wordfish scaling model and its use in estimating political positions based on text data.\n\n\n\n\nThis dataset includes tweets about the Biden–Xi summit in November 2021. We perform tokenization, Latent Semantic Analysis (LSA), and visualize hashtag and user mention networks.\n\n\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\n\n\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt &lt;- summit$text\n\n\n\n\n\ntoks &lt;- tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd=4, margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\n\n\n\n\n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\ntag_fcm &lt;- fcm(tag_dfm)\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\n\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\n\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this section, we analyse US inaugural addresses using text frequency, word clouds, keyword tracking, and keyness plots.\n\n\n\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10)\n\n\n\n\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\n\n\n\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\nfreq_american &lt;- subset(freq_grouped, feature == \"american\")\n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\nrel_freq_american &lt;- subset(rel_freq, feature == \"american\")\n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  ylab(\"Relative frequency (%)\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\ntextplot_keyness(result_keyness)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Wordfish model estimates the relative ideological positions of texts (or politicians) using word frequencies. It assumes that politicians closer in ideology will use similar language. Quanteda implements Wordfish via textmodel_wordfish().\n\n\n\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot words on dimension\ntextplot_scale1d(wf, margin = \"features\",\n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"deficit\"),\n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot party position\ntextplot_scale1d(wf, margin = \"documents\",\n                 groups = data_corpus_irishbudget2010$party)"
  },
  {
    "objectID": "assign06.html#bidenxi-summit-twitter-data",
    "href": "assign06.html#bidenxi-summit-twitter-data",
    "title": "Assignment 6",
    "section": "",
    "text": "This dataset includes tweets about the Biden–Xi summit in November 2021. We perform tokenization, Latent Semantic Analysis (LSA), and visualize hashtag and user mention networks.\n\n\n\nlibrary(quanteda)\n\nPackage version: 4.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 12 of 12 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\n\n\nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\n\nRows: 14520 Columns: 90\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (50): screen_name, text, source, reply_to_screen_name, hashtags, symbol...\ndbl  (26): user_id, status_id, display_text_width, reply_to_status_id, reply...\nlgl  (10): is_quote, is_retweet, quote_count, reply_count, ext_media_type, q...\ndttm  (4): created_at, quoted_created_at, retweet_created_at, account_create...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsum_twt &lt;- summit$text\n\n\n\n\n\ntoks &lt;- tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd=4, margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\n\n\n\n\n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\ntag_fcm &lt;- fcm(tag_dfm)\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\n\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\n\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nuser_fcm &lt;- fcm(user_dfm)\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\n\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)"
  },
  {
    "objectID": "assign06.html#us-president-inaugural-speeches",
    "href": "assign06.html#us-president-inaugural-speeches",
    "title": "Assignment 6",
    "section": "",
    "text": "In this section, we analyse US inaugural addresses using text frequency, word clouds, keyword tracking, and keyness plots.\n\n\n\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10)\n\n\n\n\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\n\n\n\n\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\ndata_corpus_inaugural_subset &lt;- corpus_subset(data_corpus_inaugural, Year &gt; 1949)\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\nfreq_american &lt;- subset(freq_grouped, feature == \"american\")\n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\nrel_freq_american &lt;- subset(rel_freq, feature == \"american\")\n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  ylab(\"Relative frequency (%)\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\ntextplot_keyness(result_keyness)"
  },
  {
    "objectID": "assign06.html#wordfish-scaling-model",
    "href": "assign06.html#wordfish-scaling-model",
    "title": "Assignment 6",
    "section": "",
    "text": "The Wordfish model estimates the relative ideological positions of texts (or politicians) using word frequencies. It assumes that politicians closer in ideology will use similar language. Quanteda implements Wordfish via textmodel_wordfish().\n\n\n\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\nwf &lt;- textmodel_wordfish(ie_dfm, dir = c(6, 5))\n\n# Plot words on dimension\ntextplot_scale1d(wf, margin = \"features\",\n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"deficit\"),\n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot party position\ntextplot_scale1d(wf, margin = \"documents\",\n                 groups = data_corpus_irishbudget2010$party)"
  },
  {
    "objectID": "assign04.html",
    "href": "assign04.html",
    "title": "Assignment 4",
    "section": "",
    "text": "Task Overview\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. The provided base code utilizes the rvest package. We modify it to scrape other tables, clean the data, and suggest a data collection plan for research.\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(stringr)\n\n\n\nRead and Scrape the Wikipedia Page\n\n# Read the web page\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\nwikiforreserve &lt;- read_html(url)\n\n# Extract the first table\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath = '//*[@id=\"mw-content-text\"]/div/table[1]') %&gt;%\n  html_table()\n\n# Convert the table to a data frame\nfores &lt;- foreignreserve[[1]]\n\n# View original column names\nnames(fores)\n\n[1] \"Country(as recognized by the U.N.)\" \"Continent\"                         \n[3] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[5] \"Foreign exchange reserves\"          \"Foreign exchange reserves\"         \n[7] \"Last reporteddate\"                  \"Ref.\"                              \n\n\n\n\nClean Up the Data\n\n# Assign consistent column names\nnames(fores) &lt;- c(\"Country\", \"Continent\", \n                  \"Forexres_InclGold\", \"Change_InclGold\", \n                  \"Forexres_ExclGold\", \"Change_ExclGold\", \n                  \"Date\", \"Ref\")\n\n# Remove junk rows and unnecessary column\nfores &lt;- fores[-c(1, 2), ]\nfores &lt;- select(fores, -Ref)\n\n# Clean date column (remove footnotes)\nfores &lt;- fores %&gt;%\n  mutate(Date = trimws(str_split_fixed(Date, \"\\\\[\", 2)[,1]))\n\n# Convert numeric columns (remove commas)\nfores &lt;- fores %&gt;%\n  mutate(across(c(Forexres_InclGold, Change_InclGold, \n                  Forexres_ExclGold, Change_ExclGold),\n                ~ as.numeric(str_remove_all(., \",\"))))\n\n# Create Rank based on Forexres_InclGold\nfores &lt;- fores %&gt;%\n  arrange(desc(Forexres_InclGold)) %&gt;%\n  mutate(Rank = row_number())\n\n# Move Rank to the first column\nfores &lt;- fores %&gt;%\n  select(Rank, everything())\n\n# Print top 10 rows with all columns visible\nprint(head(fores, 10), width = Inf)\n\n# A tibble: 10 × 8\n    Rank Country      Continent   Forexres_InclGold Change_InclGold\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;                   &lt;dbl&gt;           &lt;dbl&gt;\n 1     1 China        Asia                  3643149           41079\n 2     2 Japan        Asia                  1324210           19774\n 3     3 Switzerland  Europe                1007710           13935\n 4     4 Russia       Europe/Asia            734100           14300\n 5     5 India        Asia                   692576            5543\n 6     6 Taiwan       Asia                   597430            4390\n 7     7 Saudi Arabia Asia                   434547           21728\n 8     8 Hong Kong    Asia                   421400            5126\n 9     9 South Korea  Asia                   415700            4300\n10    10 Brazil       Americas               388571            7465\n   Forexres_ExclGold Change_ExclGold Date       \n               &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;      \n 1           3389306           31221 31 Aug 2025\n 2           1230940           16230 31 Aug 2025\n 3            897295           14490 31 Jul 2025\n 4            434487            1517 14 Nov 2025\n 5            585719             216 14 Nov 2025\n 6            544300            1071 31 Aug 2025\n 7            434116           21728 7 Nov 2024 \n 8            416216              85 8 Nov 2024 \n 9            410900             790 5 Nov 2024 \n10            344173             361 1 Oct 2024 \n\n\n\n\nSuggest a Web Data Collection Plan for Research\nThis assignment demonstrates how to scrape foreign exchange reserve data from Wikipedia using R. We use the rvest, stringr, and tidyverse packages. The focus is on extracting structured tabular data, cleaning variables such as date, and preparing it for research use.\nA simple data collection plan might involve:\n\nObjective: Identify trends in macroeconomic indicators across countries\n\nSources: Wikipedia, World Bank, IMF, TradingEconomics\n\nTools: rvest, httr, jsonlite for APIs\n\nStorage: Save into .csv or SQLite for reproducibility\n\nSchedule: Update monthly or quarterly via scheduled R scripts (using cronR or taskscheduleR)"
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "A. Google Trends Data (Manual Method)\nFor this part of the assignment, I visited the Google Trends website and searched for the terms: Trump, Kamala Harris, and Election. I set the geography to the United States and the time range to Past 5 years. After generating the trends, I downloaded the data as a CSV file.\nThe dataset contains weekly search interest scores for each of the three terms from September 2020 to the present. Each score reflects how popular a term was in a specific week, based on Google’s relative scale from 0 to 100. This scale represents how popular a term was relative to its own highest point of popularity during the period.\nFor example, a value of 100 indicates the week when the search term reached its peak popularity in the U.S. A value of 50 would mean the term was half as popular as it was at its peak. Values like 1, 2, or &lt;1 reflect very low search interest, but they don’t mean only one person searched — just that the term was relatively unpopular in that period.\nFrom the downloaded file, I noticed that:\n\n“Trump” had slightly higher interest than the other two terms during most weeks.\n“Kamala Harris” and “Election” often had very low search interest, often marked as &lt;1.\nThese trends align with expected public attention shifts before and after election periods.\n\n\n\nB. Google Trends Data (R Method)\n\nlibrary(gtrendsR)\n\nTrumpHarrisElection = gtrends(\n  c(\"Trump\", \"Harris\", \"Election\"),\n  onlyInterest = TRUE,\n  geo = \"US\",\n  gprop = \"web\",\n  time = \"today+5-y\",\n  category = 0\n) # last five years\n\nthe_df = TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\n\n\n\n\n\n\n\n\ntg = gtrends(\"Russia\", time = \"all\")\n\n# Example: Russia, Ukraine, Energy\nplot(gtrends(c(\"Russia\"), time = \"all\"))\n\n\n\n\n\n\n\ndata(\"countries\")\n\nplot(gtrends(c(\"Russia\"), geo = \"US\", time = \"all\"))\n\n\n\n\n\n\n\nplot(gtrends(c(\"Russia\"), geo = c(\"US\", \"GB\", \"CN\"), time = \"all\"))\n\n\n\n\n\n\n\ntg_iot = tg$interest_over_time\n\ntct = gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\")\ntct = data.frame(tct$interest_over_time)\n\nplot(gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\"))\n\n\n\n\n\n\n\n\n\n\nC. Saving Data\n\nwrite.csv(the_df, \"googletrends_r.csv\", row.names = FALSE)\nsaveRDS(the_df, \"TrumpHarrisElection.rds\")\n\n\n\nD. Comparing two methods\nThe manual method provides an easy interface to search terms and download CSV files. The file includes one row per week with search interest scores for each term in separate columns.\nThe R method gives a structured dataset where each row represents one term per week. It includes columns like keyword, date, and hits.\nKey differences are as follows:\n\nManual file is simpler, but not suitable for analysis.\nR method is reproducible, automatable, and better for plotting or filtering.\n\nBoth use the same 0–100 scale, but the R version is more flexible for data analysis in R."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This assignment demonstrates how to collect, map, and interpret US Census data using R. We use the tidycensus, tigris, sf, and ggplot2 packages. The focus is on median household income and poverty counts by county in California using the 2023 ACS 5-year estimates.\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n\n# 1) API key (make sure you have your Census API key!)\ncensus_api_key(\"1679bfcd12ef3ba5b6103471f96325933d49aa1f\", install = TRUE, overwrite = TRUE)\n\n[1] \"1679bfcd12ef3ba5b6103471f96325933d49aa1f\"\n\n\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n# 3) Parameters (California)\nstate_abbr &lt;- \"CA\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n# 4) Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n\n\n# 5) Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n# 6) Map median income\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = \"gray\", size = 0.1) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal() + \n  theme(\n  legend.position = c(0.99, 0.99),\n  legend.justification = c(\"right\", \"top\"),\n  legend.title = element_text(size = 11),\n  legend.text = element_text(size = 9),\n  legend.background = element_rect(fill = alpha('white', 0.6), color = NA),\n  plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n  plot.caption = element_text(hjust = 0.5, size = 9)\n)\n\n\n\n\n\n\n\n\nThe choropleth map above visualizes median household income across California counties using data from the 2023 ACS 5-year estimates. Counties are shaded using a viridis color scale, where yellow indicates the highest income levels and dark purple indicates the lowest.\nFrom the map, it is clear that the San Francisco Bay Area (including counties such as Santa Clara, San Mateo, and Marin) has the highest median household incomes, with some exceeding $150,000. These counties stand out in bright yellow. On the other hand, inland and northern counties such as those in the Central Valley or far Northern California tend to have lower median incomes, frequently under $75,000.\n\n# 7) Top and bottom counties by poverty count\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_poverty)) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_poverty) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.3423 ymin: 32.53444 xmax: -114.1312 ymax: 38.7364\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Los Angeles County, C…          1322476       15552 (((-118.6044 33.47855, -…\n 2 San Diego County, Cal…           330602        7963 (((-117.596 33.38779, -1…\n 3 Orange County, Califo…           296493        8509 (((-118.1146 33.74461, -…\n 4 San Bernardino County…           291226        9076 (((-117.8025 33.97555, -…\n 5 Riverside County, Cal…           266955        8729 (((-117.6763 33.88882, -…\n 6 Sacramento County, Ca…           197472        6775 (((-121.8625 38.06795, -…\n 7 Fresno County, Califo…           185717        5965 (((-120.9094 36.7477, -1…\n 8 Kern County, Californ…           168825        6993 (((-120.1944 35.78936, -…\n 9 Alameda County, Calif…           149752        4801 (((-122.3423 37.80556, -…\n10 Santa Clara County, C…           128470        5622 (((-122.2027 37.36305, -…\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.256 ymin: 35.78669 xmax: -115.648 ymax: 42.00076\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Alpine County, Califo…              209          98 (((-120.0724 38.70277, -…\n 2 Sierra County, Califo…              325         202 (((-121.0575 39.53999, -…\n 3 Mono County, Californ…             1441         480 (((-119.6489 38.28912, -…\n 4 Modoc County, Califor…             1717         357 (((-121.4572 41.94994, -…\n 5 Inyo County, Californ…             1928         439 (((-118.79 37.39403, -11…\n 6 Plumas County, Califo…             2075         538 (((-121.497 40.43702, -1…\n 7 Colusa County, Califo…             2332         598 (((-122.7851 39.38298, -…\n 8 Mariposa County, Cali…             2347         449 (((-120.3944 37.67504, -…\n 9 Trinity County, Calif…             2830         755 (((-123.6224 40.9317, -1…\n10 Del Norte County, Cal…             2900         589 (((-124.2175 41.95081, -…\n\n\n\n# 8) Save the data as CSV (optional)\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Musharraf Mir Akhmadova",
    "section": "",
    "text": "HELLO and WELCOME to my page. In this page you can find information about me and my assignments for the course EPPS6302."
  }
]