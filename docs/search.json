[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Musharraf Mir Akhmadova",
    "section": "",
    "text": "HELLO and WELCOME to my page. In this page you can find information about me and my assignments for the course EPPS6302."
  },
  {
    "objectID": "assign02.html",
    "href": "assign02.html",
    "title": "Assignment 2",
    "section": "",
    "text": "A. Google Trends Data (Manual Method)\nFor this part of the assignment, I visited the Google Trends website and searched for the terms: Trump, Kamala Harris, and Election. I set the geography to the United States and the time range to Past 5 years. After generating the trends, I downloaded the data as a CSV file.\nThe dataset contains weekly search interest scores for each of the three terms from September 2020 to the present. Each score reflects how popular a term was in a specific week, based on Google’s relative scale from 0 to 100. This scale represents how popular a term was relative to its own highest point of popularity during the period.\nFor example, a value of 100 indicates the week when the search term reached its peak popularity in the U.S. A value of 50 would mean the term was half as popular as it was at its peak. Values like 1, 2, or &lt;1 reflect very low search interest, but they don’t mean only one person searched — just that the term was relatively unpopular in that period.\nFrom the downloaded file, I noticed that:\n\n“Trump” had slightly higher interest than the other two terms during most weeks.\n“Kamala Harris” and “Election” often had very low search interest, often marked as &lt;1.\nThese trends align with expected public attention shifts before and after election periods.\n\n\n\nB. Google Trends Data (R Method)\n\nlibrary(gtrendsR)\n\nTrumpHarrisElection = gtrends(\n  c(\"Trump\", \"Harris\", \"Election\"),\n  onlyInterest = TRUE,\n  geo = \"US\",\n  gprop = \"web\",\n  time = \"today+5-y\",\n  category = 0\n) # last five years\n\nthe_df = TrumpHarrisElection$interest_over_time\nplot(TrumpHarrisElection)\n\n\n\n\n\n\n\n\n\ntg = gtrends(\"Russia\", time = \"all\")\n\n# Example: Russia, Ukraine, Energy\nplot(gtrends(c(\"Russia\"), time = \"all\"))\n\n\n\n\n\n\n\ndata(\"countries\")\n\nplot(gtrends(c(\"Russia\"), geo = \"US\", time = \"all\"))\n\n\n\n\n\n\n\nplot(gtrends(c(\"Russia\"), geo = c(\"US\", \"GB\", \"CN\"), time = \"all\"))\n\n\n\n\n\n\n\ntg_iot = tg$interest_over_time\n\ntct = gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\")\ntct = data.frame(tct$interest_over_time)\n\nplot(gtrends(c(\"Russia\", \"Ukraine\", \"War\"), time = \"all\"))\n\n\n\n\n\n\n\n\n\n\nC. Saving Data\n\nwrite.csv(the_df, \"googletrends_r.csv\", row.names = FALSE)\nsaveRDS(the_df, \"TrumpHarrisElection.rds\")\n\n\n\nD. Comparing two methods\nThe manual method provides an easy interface to search terms and download CSV files. The file includes one row per week with search interest scores for each term in separate columns.\nThe R method gives a structured dataset where each row represents one term per week. It includes columns like keyword, date, and hits.\nKey differences are as follows:\n\nManual file is simpler, but not suitable for analysis.\nR method is reproducible, automatable, and better for plotting or filtering.\n\nBoth use the same 0–100 scale, but the R version is more flexible for data analysis in R."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Musharraf Mir Akhmadova, and I am a second year PhD student in PPPE program. I am interested in doing research and my research interests are gender economics, child nutrition status and global health.\nBesides my academic journey, I am also a new mom to a 9-month baby girl named Fatima. Fatima motivates me not to give up at times when I feel tired during my studies."
  },
  {
    "objectID": "assign01.html",
    "href": "assign01.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Website Design Reflection\nThis website was created using Quarto in RStudio as part of the EPPS6302 course. The Quarto Website feature allows for simple yet powerful publishing of professional web pages directly from .qmd files.\nThe website uses the Cosmo theme for a clean and professional appearance. A custom styles.css file is also included to enable additional design control if needed in the future.\nThe navigation bar at the top includes:\n\nHome: the landing page of the website\nAbout: a short bio\nResume: links directly to my uploaded CV in PDF format\nAssignments: links to course assignments like this one\n\nThe site is configured using the _quarto.yml file, which defines layout, theme, and navigation structure.\nThe purpose of this website is to showcase my course progress, academic credentials, and assignments in a structured and accessible way.\n\n\nPodcast Review\nIn this episode of the DataFramed podcast, Richie Cotton talks with Karen Ng, who is the Head of Product at HubSpot. They discuss how humans and AI tools can work together in modern workplaces. Karen has worked at major tech companies like Microsoft and Google, so she brings a lot of experience to the conversation.\nOne of the main ideas in the episode is that not all AI tools are the same. Karen explains the differences between chatbots, co-pilots, and agents:\n\nChatbots are simple tools that answer basic questions.\nCo-pilots are AI systems that support you while you work, like giving suggestions or completing part of a task.\nAgents are more advanced. They can take a task, complete it from start to finish, and act more independently.\n\nKaren believes that AI is here to help humans, not replace them. She says, “Humans lead, and AI accelerates.” This means people should make the important decisions, and AI can help make things faster or easier. For example, if someone works in customer service, AI can help by answering common questions, so the human worker can focus on harder problems.\nShe also talks about how companies should introduce AI step by step. She suggests starting small, testing AI tools on simple tasks, and then checking if they actually make things better. Karen says it’s important to use your company’s own data to train the AI. That way, it understands your work better.\nAnother interesting idea she mentions is called AI Engine Optimization (AEO). It’s similar to SEO (Search Engine Optimization), but instead of helping people find your content through Google, AEO helps AI systems understand and use your content. This is a new idea, but it’s becoming more important as AI tools like ChatGPT and Google Bard are used to answer questions and summarize content.\nWhat I liked about this episode is how clear and practical it was. Even though AI is a big topic, Karen explains it in a simple way. She doesn’t use too much technical language, and she gives good examples that are easy to understand. I also appreciated that she focused on how AI can help real workers, not just replace them.\nOverall, I learned a lot from this podcast. It made me feel more comfortable with the idea of using AI in workplaces. I would recommend this episode to anyone who wants to understand how people and AI can work together in smart and helpful ways — especially in jobs like customer service, sales, or marketing."
  },
  {
    "objectID": "assign03.html",
    "href": "assign03.html",
    "title": "Assignment 3",
    "section": "",
    "text": "This assignment demonstrates how to collect, map, and interpret US Census data using R. We use the tidycensus, tigris, sf, and ggplot2 packages. The focus is on median household income and poverty counts by county in California using the 2023 ACS 5-year estimates.\n\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\noptions(tigris_use_cache = TRUE)\n\n\n# 1) API key (make sure you have your Census API key!)\ncensus_api_key(\"1679bfcd12ef3ba5b6103471f96325933d49aa1f\", install = TRUE, overwrite = TRUE)\n\n[1] \"1679bfcd12ef3ba5b6103471f96325933d49aa1f\"\n\n\n\n# 2) Explore variables\nvars &lt;- load_variables(2023, \"acs5\", cache = TRUE)\nvars |&gt; dplyr::filter(grepl(\"^B19\", name)) |&gt; dplyr::slice_head(n = 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n# 3) Parameters (California)\nstate_abbr &lt;- \"CA\"\ngeo_level  &lt;- \"county\"   # options: state, county, tract, block group\nmy_vars    &lt;- c(income = \"B19013_001\", poverty = \"B17001_002\")\nyear_acs   &lt;- 2023\nsurvey     &lt;- \"acs5\"\n\n\n# 4) Download ACS data with geometry\nacs &lt;- get_acs(\n  geography = geo_level,\n  variables = my_vars,\n  state = state_abbr,\n  year = year_acs,\n  survey = survey,\n  geometry = TRUE\n)\n\nGetting data from the 2019-2023 5-year ACS\n\n\n\n# 5) Convert to wide format\nacs_wide &lt;- acs |&gt;\n  tidyr::pivot_wider(\n    id_cols = c(GEOID, NAME, geometry),\n    names_from = variable,\n    values_from = c(estimate, moe)\n  )\n\n\n# 6) Map median income\nggplot(acs_wide) +\n  geom_sf(aes(fill = estimate_income), color = \"gray\", size = 0.1) +\n  scale_fill_viridis_c(name = \"Median HH Income\") +\n  labs(title = paste0(\"ACS \", year_acs, \" 5-year: Median Income — \", state_abbr, \" (\", geo_level, \")\"),\n       caption = \"Source: U.S. Census Bureau via tidycensus\") +\n  theme_minimal() + \n  theme(\n  legend.position = c(0.99, 0.99),\n  legend.justification = c(\"right\", \"top\"),\n  legend.title = element_text(size = 11),\n  legend.text = element_text(size = 9),\n  legend.background = element_rect(fill = alpha('white', 0.6), color = NA),\n  plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n  plot.caption = element_text(hjust = 0.5, size = 9)\n)\n\n\n\n\n\n\n\n\nThe choropleth map above visualizes median household income across California counties using data from the 2023 ACS 5-year estimates. Counties are shaded using a viridis color scale, where yellow indicates the highest income levels and dark purple indicates the lowest.\nFrom the map, it is clear that the San Francisco Bay Area (including counties such as Santa Clara, San Mateo, and Marin) has the highest median household incomes, with some exceeding $150,000. These counties stand out in bright yellow. On the other hand, inland and northern counties such as those in the Central Valley or far Northern California tend to have lower median incomes, frequently under $75,000.\n\n# 7) Top and bottom counties by poverty count\ntop10 &lt;- acs_wide |&gt;\n  arrange(desc(estimate_poverty)) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\nbottom10 &lt;- acs_wide |&gt;\n  arrange(estimate_poverty) |&gt;\n  select(NAME, estimate_poverty, moe_poverty) |&gt;\n  slice_head(n = 10)\n\ntop10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -122.3423 ymin: 32.53444 xmax: -114.1312 ymax: 38.7364\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Los Angeles County, C…          1322476       15552 (((-118.6044 33.47855, -…\n 2 San Diego County, Cal…           330602        7963 (((-117.596 33.38779, -1…\n 3 Orange County, Califo…           296493        8509 (((-118.1146 33.74461, -…\n 4 San Bernardino County…           291226        9076 (((-117.8025 33.97555, -…\n 5 Riverside County, Cal…           266955        8729 (((-117.6763 33.88882, -…\n 6 Sacramento County, Ca…           197472        6775 (((-121.8625 38.06795, -…\n 7 Fresno County, Califo…           185717        5965 (((-120.9094 36.7477, -1…\n 8 Kern County, Californ…           168825        6993 (((-120.1944 35.78936, -…\n 9 Alameda County, Calif…           149752        4801 (((-122.3423 37.80556, -…\n10 Santa Clara County, C…           128470        5622 (((-122.2027 37.36305, -…\n\nbottom10\n\nSimple feature collection with 10 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -124.256 ymin: 35.78669 xmax: -115.648 ymax: 42.00076\nGeodetic CRS:  NAD83\n# A tibble: 10 × 4\n   NAME                   estimate_poverty moe_poverty                  geometry\n   &lt;chr&gt;                             &lt;dbl&gt;       &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n 1 Alpine County, Califo…              209          98 (((-120.0724 38.70277, -…\n 2 Sierra County, Califo…              325         202 (((-121.0575 39.53999, -…\n 3 Mono County, Californ…             1441         480 (((-119.6489 38.28912, -…\n 4 Modoc County, Califor…             1717         357 (((-121.4572 41.94994, -…\n 5 Inyo County, Californ…             1928         439 (((-118.79 37.39403, -11…\n 6 Plumas County, Califo…             2075         538 (((-121.497 40.43702, -1…\n 7 Colusa County, Califo…             2332         598 (((-122.7851 39.38298, -…\n 8 Mariposa County, Cali…             2347         449 (((-120.3944 37.67504, -…\n 9 Trinity County, Calif…             2830         755 (((-123.6224 40.9317, -1…\n10 Del Norte County, Cal…             2900         589 (((-124.2175 41.95081, -…\n\n\n\n# 8) Save the data as CSV (optional)\nreadr::write_csv(st_drop_geometry(acs_wide), \"acs_data.csv\")"
  }
]